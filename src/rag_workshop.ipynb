{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d4a6e0",
   "metadata": {},
   "source": [
    "# RAG Workshop: Building a Retrieval-Augmented QA System with Azure OpenAI\n",
    "\n",
    "Welcome! This notebook walks through an end‚Äëto‚Äëend mini Retrieval Augmented Generation (RAG) pipeline using:\n",
    "- Azure OpenAI (chat + embeddings)\n",
    "- LangChain document loaders & text splitter\n",
    "- FAISS in‚Äëmemory vector store\n",
    "- A nutrition PDF as our knowledge base\n",
    "\n",
    "Workshop Flow:\n",
    "1. Environment & credentials\n",
    "2. Acquire source document (PDF)\n",
    "3. Load & split into semantic chunks\n",
    "4. Embed chunks + build vector index\n",
    "5. Inspect an embedding (intuition)\n",
    "6. Perform retrieval + grounded chat completion\n",
    "7. Discuss next steps & enhancements\n",
    "\n",
    "Learning Goals:\n",
    "- Understand why chunking matters\n",
    "- See how embeddings enable similarity search\n",
    "- Learn prompt grounding basics\n",
    "- Know where to optimize / productionize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d5fe3",
   "metadata": {},
   "source": [
    "### 1. Environment & Configuration\n",
    "This cell:\n",
    "- Locates a `.env` file in the project root\n",
    "- Loads required Azure OpenAI + embedding deployment variables\n",
    "- Verifies presence of critical values before continuing\n",
    "\n",
    "Why it matters:\n",
    "Without correct credentials, subsequent API calls (embeddings + chat completions) will fail. Early validation shortens debug time.\n",
    "\n",
    "Checklist (before running):\n",
    "- `.env` exists one level up from `src/`\n",
    "- Contains: `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_VERSION`, `AZURE_OPENAI_MODEL`, optional `AZURE_EMBEDDING_DEPLOYMENT`, `AZURE_DEPLOYMENT`\n",
    "\n",
    "If you see missing variables: open `.env`, add them, re-run this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ff228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "env_path = os.path.join('..', '.env')\n",
    "\n",
    "# 1. Loading the variables\n",
    "if os.path.exists(env_path):\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "    print(f\"‚úÖ Loaded configuration from: {os.path.abspath(env_path)}\")\n",
    "else:\n",
    "    print(\"‚ùå Error: .env file not found in project root.\")\n",
    "\n",
    "# 2. Verifying Credentials\n",
    "required_vars = [\n",
    "    \"AZURE_OPENAI_API_KEY\", \n",
    "    \"AZURE_OPENAI_ENDPOINT\", \n",
    "    \"AZURE_OPENAI_API_VERSION\", \n",
    "    \"AZURE_OPENAI_MODEL\",\n",
    "]\n",
    "\n",
    "missing = [var for var in required_vars if not os.getenv(var)]\n",
    "if missing:\n",
    "    print(f\"‚ùå Missing environment variables: {missing}\")\n",
    "else:\n",
    "    key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    print(f\"‚úÖ Azure Configured. Key: {key[:5]}...******\")\n",
    "    print(f\"‚úÖ Azure OpenAI Model: {os.getenv('AZURE_OPENAI_MODEL')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2b55f",
   "metadata": {},
   "source": [
    "### 2. Source Document Acquisition (PDF Download)\n",
    "Goal:\n",
    "Fetch a public PDF once and store it locally so downstream steps operate on a stable artifact instead of re-hitting the network.\n",
    "\n",
    "Key Points:\n",
    "- Checks if the file already exists to avoid redundant downloads.\n",
    "- Saves into `../pdf/` relative to this notebook.\n",
    "\n",
    "Why Local Storage?\n",
    "Repeated parsing of remote documents increases latency and introduces failure points. Caching locally standardizes the pipeline for every participant.\n",
    "\n",
    "Try:\n",
    "Delete the file and re-run to see the download logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a29ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os \n",
    "\n",
    "# 1. Configuration\n",
    "pdf_url = \"https://globalwellnessinstitute.org/wp-content/uploads/2023/12/NUTRITION_4_HEALTH_SPAN_GWI_final_202301210_hi-res.pdf\"\n",
    "\n",
    "output_folder = \"../pdf\"\n",
    "\n",
    "file_path = os.path.join(output_folder, \"nutrition_healthspan.pdf\")\n",
    "\n",
    "# 2. Download if not exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"Downloading PDF from {pdf_url}...\")\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(pdf_url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"‚úÖ Download complete.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download. Status: {response.status_code}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è File already exists locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65725058",
   "metadata": {},
   "source": [
    "### 3. Loading & Chunking the Document\n",
    "This step:\n",
    "1. Loads the PDF pages into memory using `PyPDFLoader`.\n",
    "2. Applies `RecursiveCharacterTextSplitter` to create overlapping chunks.\n",
    "\n",
    "Why Chunking?\n",
    "- LLM context windows are limited; we can't pass the entire PDF.\n",
    "- Smaller semantic units improve retrieval precision.\n",
    "- Overlap (`chunk_overlap`) preserves continuity (avoids cutting sentences abruptly).\n",
    "\n",
    "Parameters:\n",
    "- `chunk_size=1000`: Tune for model/context size; too large reduces recall granularity.\n",
    "- `chunk_overlap=200`: Helps maintain context for boundary sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ef965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"Loading PDF... (This reads the file)\")\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "print(f\"   Loaded {len(docs)} pages.\")\n",
    "\n",
    "# Spliting Configuration\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,    # Characters per chunk\n",
    "    chunk_overlap=200   # Overlap to preserve context\n",
    ")\n",
    "\n",
    "print(\"Splitting document into chunks...\")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"‚úÖ Created {len(splits)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303663cb",
   "metadata": {},
   "source": [
    "### 4. Embeddings & Vector Index Construction\n",
    "Purpose:\n",
    "Transform textual chunks into high-dimensional vectors so we can perform similarity search (find the most relevant passages for a query).\n",
    "\n",
    "Components:\n",
    "- `AzureOpenAIEmbeddings`: Calls Azure deployment for embedding generation.\n",
    "- `FAISS.from_documents`: Builds an in-memory index optimized for fast nearest-neighbor lookup.\n",
    "\n",
    "Key Parameters:\n",
    "- Model: `text-embedding-3-small` (1536 dims) ‚Äî balance of cost vs. semantic fidelity.\n",
    "- `chunk_size`: Batches requests; too large can hit rate limits, too small may slow throughput.\n",
    "- Retry settings: Helpful for transient network/API issues.\n",
    "\n",
    "Why FAISS?\n",
    "- It is a local, in-memory library and does not require a separate client or server to operate.\n",
    "- Efficient vector similarity search\n",
    "\n",
    "Alternative Stores:\n",
    "- Chroma, Opensearch, Pinecone, etc.\n",
    "\n",
    "After Running:\n",
    "You have a retrievable knowledge base ready for RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4993ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "# 1. Configuring Embedding Model\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT\"),    \n",
    "    chunk_size=100,\n",
    "    show_progress_bar=True,\n",
    "    max_retries=20,\n",
    "    retry_min_seconds=2\n",
    ")\n",
    "\n",
    "print(f\"‚öóÔ∏è  Embedding chunks using: {os.getenv('AZURE_EMBEDDING_DEPLOYMENT')}...\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "print(\"‚úÖ Vector Store created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49229a3",
   "metadata": {},
   "source": [
    "### 5. Inspecting a Sample Embedding\n",
    "Objective:\n",
    "Demystify what an embedding looks like and confirm expected dimensionality.\n",
    "\n",
    "What Happens:\n",
    "- We call `embed_query` on a simple token (`\"apple\"`).\n",
    "- Print metadata: length (vector size), type, first few values.\n",
    "\n",
    "Why Inspect?\n",
    "- Quick sanity check to ensure the embedding model is working.\n",
    "- Dimension mismatch is a common integration error when swapping models.\n",
    "\n",
    "Notes:\n",
    "- Values have no human-readable meaning individually.\n",
    "- Similar words will produce vectors closer in cosine space.\n",
    "\n",
    "Try:\n",
    "Replace `sample_word` with terms like `fruit`, `nutrition`, `longevity` and compare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1316f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample embedding\n",
    "sample_word = \"apple\"\n",
    "vector = embeddings.embed_query(sample_word)\n",
    "\n",
    "print(f\"Word: '{sample_word}'\")\n",
    "print(f\"Vector Dimensions: {len(vector)}\") # Should be 1536 for OpenAI models\n",
    "print(f\"Type: {type(vector)}\")\n",
    "print(f\"First 10 numbers: {vector[:10]}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6fe0d",
   "metadata": {},
   "source": [
    "### 6. Retrieval-Augmented Generation (RAG) Query Flow\n",
    "\n",
    "Overview:\n",
    "1. Accept a user question interactively.\n",
    "2. Perform vector similarity search (`k=3`) to fetch top relevant chunks.\n",
    "3. Build a grounded prompt: strict system instructions + context + user question.\n",
    "4. Call Azure Chat Completion model using deployment configured in env.\n",
    "5. Display the model's answer.\n",
    "\n",
    "Why k=3?\n",
    "Small k keeps prompt size manageable while offering multiple perspectives. Tune based on chunk size & model context window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33167cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os \n",
    "\n",
    "# 1. Setup Azure Client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "deployment = os.getenv(\"AZURE_DEPLOYMENT\")\n",
    "\n",
    "# 2. Define Question\n",
    "user_question = input(\"Enter your question: \")\n",
    "\n",
    "# 3. Retrieve Context (Manually)\n",
    "print(f\"üîç Searching PDF for: '{user_question}'...\")\n",
    "relevant_docs = vectorstore.similarity_search(user_question, k=3)\n",
    "\n",
    "# Join the retrieved text into one big string\n",
    "context_data = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "# 4. Preparing the Prompt\n",
    "system_prompt = \"\"\"You are a strict assistant.\n",
    "Your ONLY task is to answer the user's question based on the provided context below.\n",
    "- Do NOT use your internal knowledge.\n",
    "- Do NOT make up facts.\n",
    "- If the answer is not explicitly written in the context, you MUST say \"I don't know\".\n",
    "- Do not try to be helpful by adding outside information.\n",
    "\"\"\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "Context:\n",
    "{context_data}\n",
    "\n",
    "Question: \n",
    "{user_question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Call GPT-5 \n",
    "print(f\"ü§ñ Asking Azure {os.getenv('AZURE_OPENAI_MODEL')}...\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 6. Output\n",
    "print(\"\\n--- Answer ---\")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
